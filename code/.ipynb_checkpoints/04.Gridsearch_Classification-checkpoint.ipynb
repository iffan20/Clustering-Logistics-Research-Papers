{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3d5c24-d9a1-4825-84dd-afddc486c569",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 100px\">\n",
    "\n",
    "# Capstone Project: Classifying Logistics Research Papers\n",
    "## Part 4 : Gridsearch Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413125f9-5bce-4d60-94fb-fc2bef68276d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    " [Part 1: Get Text](01.Get_Text.ipynb) | [Part 2: Add Label](02.Add_Label.ipynb) | [Part 3: EDA](03.EDA.ipynb) | **Part 4: Gridsearch Classification** | [Part 5: Neural Network Classification](05.NeuralNet_Classification.ipynb) | [Part 6: Model Evaluation](06.Model_Evaluation.ipynb) | [Part 7: Final Model](07.Final_Model.ipynb) \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab6605-d29a-4a55-bbb3-6b013a9375e2",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook focuses on the model tuning and optimization process for a text classification task. Specifically, we aim to identify the best-performing model by applying GridSearchCV to systematically explore hyperparameter combinations for the following machine learning algorithms:\n",
    "\n",
    "1. **Support Vector Machine (SVM)**\n",
    "2. **Naive Bayes (NB)**\n",
    "3. **Gradient Boosting (GB)**\n",
    "\n",
    "To preprocess the text data, we use **TfidfVectorizer** as the vectorizer, which converts text into numerical features by calculating the Term Frequency-Inverse Document Frequency (TF-IDF) scores for each word. The TfidfVectorizer is further tuned as part of the GridSearch process by adjusting its key hyperparameters, such as `max_features` and `max_df`, to optimize the quality and efficiency of the text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af72bbe-d98a-464a-98d1-baa572db9e64",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3beae1d8-8766-4b35-810a-7af88ca13485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus.common import thai_words, thai_stopwords\n",
    "from pythainlp.util import dict_trie\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9650acd3-89ee-4fec-9345-f6a2ddde8d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>abstract</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>multi_category</th>\n",
       "      <th>keywords</th>\n",
       "      <th>category_id</th>\n",
       "      <th>abstract_length</th>\n",
       "      <th>content_length</th>\n",
       "      <th>content_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>การจัดทำคู่มือขั้นตอนการดำเนินการการนำเข้าคราฟ...</td>\n",
       "      <td>ผู้วิจัยได้ตระหนักถึงความยุ่งยากของขั้นตอนการน...</td>\n",
       "      <td>การจัดทำคู่มือขั้นตอนการดำเนินการการนำเข้าคราฟ...</td>\n",
       "      <td>Import-Export and International Trade</td>\n",
       "      <td>{1: 'Import-Export and International Trade', 2...</td>\n",
       "      <td>คู่มือการนำเข้าคราฟท์เบียร์, การดำเนินงานตามมา...</td>\n",
       "      <td>5</td>\n",
       "      <td>859</td>\n",
       "      <td>11582</td>\n",
       "      <td>2402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>การเสนอแนวทางในการพัฒนาและสร้างความสัมพันธ์กับ...</td>\n",
       "      <td>งานวิจัยครั้งนี้มีวัตถุประสงค์เพื่อเสนอแนวทางใ...</td>\n",
       "      <td>การเสนอแนวทางในการพัฒนาและสร้างความสัมพันธ์กับ...</td>\n",
       "      <td>Procurement</td>\n",
       "      <td>{1: 'Procurement', 2: 'Manufacturing/Productio...</td>\n",
       "      <td>การประเมินการปฏิบัติงาน, ผู้ส่งมอบ, แบ่งเกรด, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1172</td>\n",
       "      <td>15230</td>\n",
       "      <td>3601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>การพัฒนามาตรฐานรถขนส่งวัตถุอันตรายที่เข้ามาในค...</td>\n",
       "      <td>ดำเนินธุรกิจเป็นผู้นำเข้า และจัดจำหน่ายสินค้าก...</td>\n",
       "      <td>การพัฒนามาตรฐานรถขนส่งวัตถุอันตรายที่เข้ามาในค...</td>\n",
       "      <td>Logistics and Distribution</td>\n",
       "      <td>{1: 'Logistics and Distribution', 2: 'Inventor...</td>\n",
       "      <td>Chemical Solvent, การควบคุมความปลอดภัย, รถขนส่...</td>\n",
       "      <td>3</td>\n",
       "      <td>1964</td>\n",
       "      <td>13587</td>\n",
       "      <td>2883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>แนวทางการปรับปรุงกระบวนการการส่งเอกสารใบกำกับภ...</td>\n",
       "      <td>การวิจัยครั้งนี้มีวัตถุประสงค์ เพื่อศึกษาขั้นต...</td>\n",
       "      <td>แนวทางการปรับปรุงกระบวนการการส่งเอกสารใบกำกับภ...</td>\n",
       "      <td>Procurement</td>\n",
       "      <td>{1: 'Procurement', 2: 'Demand Planning and For...</td>\n",
       "      <td>การวิจัย, กระบวนการจัดส่งใบกำกับภาษี, แผนกบัญช...</td>\n",
       "      <td>0</td>\n",
       "      <td>1252</td>\n",
       "      <td>13124</td>\n",
       "      <td>3283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>การศึกษาเทคนิคการพยากรณ์ยอดขายสายไฟที่เหมาะสม</td>\n",
       "      <td>จากสถานการณ์การแพร่ระบาดของเชื้อไวรัสโคโรนา 20...</td>\n",
       "      <td>การศึกษาเทคนิคการพยากรณ์ยอดขายสายไฟที่เหมาะสม ...</td>\n",
       "      <td>Demand Planning and Forecasting</td>\n",
       "      <td>{1: 'Demand Planning and Forecasting', 2: 'Inv...</td>\n",
       "      <td>โควิด-19, ยานยนต์, การพยากรณ์ยอดขาย, Simple mo...</td>\n",
       "      <td>4</td>\n",
       "      <td>1924</td>\n",
       "      <td>25247</td>\n",
       "      <td>6114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             project  \\\n",
       "0  การจัดทำคู่มือขั้นตอนการดำเนินการการนำเข้าคราฟ...   \n",
       "1  การเสนอแนวทางในการพัฒนาและสร้างความสัมพันธ์กับ...   \n",
       "2  การพัฒนามาตรฐานรถขนส่งวัตถุอันตรายที่เข้ามาในค...   \n",
       "3  แนวทางการปรับปรุงกระบวนการการส่งเอกสารใบกำกับภ...   \n",
       "4      การศึกษาเทคนิคการพยากรณ์ยอดขายสายไฟที่เหมาะสม   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  ผู้วิจัยได้ตระหนักถึงความยุ่งยากของขั้นตอนการน...   \n",
       "1  งานวิจัยครั้งนี้มีวัตถุประสงค์เพื่อเสนอแนวทางใ...   \n",
       "2  ดำเนินธุรกิจเป็นผู้นำเข้า และจัดจำหน่ายสินค้าก...   \n",
       "3  การวิจัยครั้งนี้มีวัตถุประสงค์ เพื่อศึกษาขั้นต...   \n",
       "4  จากสถานการณ์การแพร่ระบาดของเชื้อไวรัสโคโรนา 20...   \n",
       "\n",
       "                                             content  \\\n",
       "0  การจัดทำคู่มือขั้นตอนการดำเนินการการนำเข้าคราฟ...   \n",
       "1  การเสนอแนวทางในการพัฒนาและสร้างความสัมพันธ์กับ...   \n",
       "2  การพัฒนามาตรฐานรถขนส่งวัตถุอันตรายที่เข้ามาในค...   \n",
       "3  แนวทางการปรับปรุงกระบวนการการส่งเอกสารใบกำกับภ...   \n",
       "4  การศึกษาเทคนิคการพยากรณ์ยอดขายสายไฟที่เหมาะสม ...   \n",
       "\n",
       "                                category  \\\n",
       "0  Import-Export and International Trade   \n",
       "1                            Procurement   \n",
       "2             Logistics and Distribution   \n",
       "3                            Procurement   \n",
       "4        Demand Planning and Forecasting   \n",
       "\n",
       "                                      multi_category  \\\n",
       "0  {1: 'Import-Export and International Trade', 2...   \n",
       "1  {1: 'Procurement', 2: 'Manufacturing/Productio...   \n",
       "2  {1: 'Logistics and Distribution', 2: 'Inventor...   \n",
       "3  {1: 'Procurement', 2: 'Demand Planning and For...   \n",
       "4  {1: 'Demand Planning and Forecasting', 2: 'Inv...   \n",
       "\n",
       "                                            keywords  category_id  \\\n",
       "0  คู่มือการนำเข้าคราฟท์เบียร์, การดำเนินงานตามมา...            5   \n",
       "1  การประเมินการปฏิบัติงาน, ผู้ส่งมอบ, แบ่งเกรด, ...            0   \n",
       "2  Chemical Solvent, การควบคุมความปลอดภัย, รถขนส่...            3   \n",
       "3  การวิจัย, กระบวนการจัดส่งใบกำกับภาษี, แผนกบัญช...            0   \n",
       "4  โควิด-19, ยานยนต์, การพยากรณ์ยอดขาย, Simple mo...            4   \n",
       "\n",
       "   abstract_length  content_length  content_word_count  \n",
       "0              859           11582                2402  \n",
       "1             1172           15230                3601  \n",
       "2             1964           13587                2883  \n",
       "3             1252           13124                3283  \n",
       "4             1924           25247                6114  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned_text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946dc163-066a-4bb7-bd6b-d0fe9064ba18",
   "metadata": {},
   "source": [
    "### Text Preprocessing with Thai-Specific Methods\n",
    "\n",
    "\n",
    "Thai-specific text preprocessing methods for Natural Language Processing (NLP) tasks. It includes:\n",
    "\n",
    "- `Traditional Preprocessing` using stopword removal and tokenization tailored for Thai text.\n",
    "- `WangchanBERTa` Tokenization, leveraging a pre-trained Thai language model for deep learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4368e48a-9175-43d4-8232-1147cb018130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom word to keep from 5 sample abstract\n",
    "added_words = ['การนำเข้า', 'ฐานนิยม', 'คราฟท์', 'แนวทาง', 'ผู้ส่งมอบ', 'โซ่อุปทาน', 'ปัจจัยรอง', \n",
    "               'การส่งมอบ', 'รถขนส่ง', 'นำไปใช้งาน', 'อย่างถูกต้อง', 'การขับรถ', 'ที่เกี่ยวข้อง', \n",
    "               'ในการปฏิบัติงาน', 'พนักงานขับรถ', 'สิ่งสำคัญ', 'ขั้นตอน', 'ที่ชัดเจน', 'การไหล', 'ยอดขาย', \n",
    "              'การจัดทำ', 'คราฟท์เบียร์', 'ฝึกสหกิจ', 'อย่างก้าวกระโดด','การจัดซื้อจัดหา','กระบวนการ',\n",
    "               'แบบประเมิน','เก็บข้อมูล','อย่างชัดเจน','การดำเนินการ','การส่งเสริม','ถังดับเพลิง','แนวทาง']\n",
    "\n",
    "# Merge custom words with Thai dictionary words\n",
    "custom_words = set(thai_words()).union(added_words)\n",
    "custom_trie = dict_trie(custom_words)  # Create a trie from the custom dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7239a80e-1fa3-41da-a27c-b677e7c481db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thai_preprocess(text):\n",
    "    stopwords = thai_stopwords()\n",
    "    tokens = word_tokenize(text, custom_dict= custom_trie, engine=\"newmm\")\n",
    "    return \" \".join([token for token in tokens if token not in stopwords])\n",
    "\n",
    "\n",
    "def wangchan(text):   \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5d939-9bec-4c08-b759-dc77a137d6c2",
   "metadata": {},
   "source": [
    "###  GridSearch\n",
    "**Setting Vectorizer Parameters** : We tune the TfidfVectorizer by optimizing the following two hyperparameters:\n",
    "\n",
    "1. `max_features` : Set to 5000, 7000, or None.\n",
    "2. `max_df` : Set to 0.8 or 0.9.\n",
    "\n",
    "Given that our dataset contains approximately 10,000 features per file, reducing the number of features may improve the model's performance by focusing on the most informative terms and reducing noise. This step is crucial for balancing model complexity and computational efficiency.\n",
    "\n",
    "**Models in GridSearch** : We apply GridSearchCV to three models:\n",
    "\n",
    "1. `Support Vector Machine (SVM)` : SVM is effective in high-dimensional spaces and works well when there's a clear margin of separation between classes.\n",
    "\n",
    "2. `Naive Bayes` : It's a simple, probabilistic model that works well with high-dimensional data, offering fast and efficient performance for text classification.\n",
    "\n",
    "3. `Gradient Boosting` :It builds strong models through an ensemble of decision trees, capturing complex patterns and non-linear relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b99688fb-5025-40dd-bddb-943eb6b82b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_train, X_test, y_train, y_test, tokenizer):\n",
    "    # Define pipelines for each classifier\n",
    "    pipelines = {\n",
    "        'svm': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('classifier', SVC())\n",
    "        ]),\n",
    "        'naive_bayes': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ]),\n",
    "        'gradient_boosting': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('classifier', GradientBoostingClassifier())\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Define parameter grids for each classifier\n",
    "    param_grids = {\n",
    "        'svm': {\n",
    "            'tfidf__max_features': [5000, 7000, None],\n",
    "            'tfidf__max_df': [0.8, 0.9],\n",
    "            'classifier__C': [1, 10, 100],  # Regularization parameter\n",
    "            'classifier__kernel': ['linear', 'rbf']  # Kernel\n",
    "        },\n",
    "        'naive_bayes': {\n",
    "            'tfidf__max_features': [5000, 7000, None],\n",
    "            'tfidf__max_df': [0.8, 0.9],\n",
    "            'classifier__alpha': [0.01, 0.1, 1.0]  # Smoothing parameter\n",
    "        },\n",
    "        'gradient_boosting': {\n",
    "            'tfidf__max_features': [5000, 7000, None],\n",
    "            'tfidf__max_df': [0.8, 0.9],\n",
    "            'classifier__n_estimators': [100, 200],  # Number of trees\n",
    "            'classifier__learning_rate': [0.01, 0.1, 0.2],  # Learning rate\n",
    "            'classifier__max_depth': [3, 5, 7]  # Max depth of trees\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Store results for all classifiers\n",
    "    all_results = []\n",
    "    \n",
    "    for model_name, pipeline in pipelines.items():\n",
    "        print(f\"Running GridSearch for {model_name}...\")\n",
    "        grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"Best Parameters for {model_name}:\", grid_search.best_params_)\n",
    "        print(f\"Best Cross-Validation Accuracy for {model_name}:\", grid_search.best_score_)\n",
    "        \n",
    "        # Predict on test data\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Save results\n",
    "        result = pd.DataFrame(grid_search.cv_results_)\n",
    "        result['classifier'] = model_name\n",
    "        result['tokenizer'] = tokenizer.__name__\n",
    "        all_results.append(result)\n",
    "    \n",
    "    # Combine results from all classifiers\n",
    "    combined_results = pd.concat(all_results, ignore_index=True)\n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c300f62-7517-476c-a571-c9adeeb03737",
   "metadata": {},
   "source": [
    "### Tuning the model and collect into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c8d2813-acf8-46f8-b81f-3aacf62714f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearch for svm...\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best Parameters for svm: {'classifier__C': 10, 'classifier__kernel': 'linear', 'tfidf__max_df': 0.8, 'tfidf__max_features': 5000}\n",
      "Best Cross-Validation Accuracy for svm: 0.5632383966244725\n",
      "Running GridSearch for naive_bayes...\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Best Parameters for naive_bayes: {'classifier__alpha': 0.01, 'tfidf__max_df': 0.8, 'tfidf__max_features': 5000}\n",
      "Best Cross-Validation Accuracy for naive_bayes: 0.5377637130801688\n",
      "Running GridSearch for gradient_boosting...\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "Best Parameters for gradient_boosting: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 200, 'tfidf__max_df': 0.9, 'tfidf__max_features': 7000}\n",
      "Best Cross-Validation Accuracy for gradient_boosting: 0.5504219409282701\n",
      "Running GridSearch for svm...\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best Parameters for svm: {'classifier__C': 10, 'classifier__kernel': 'linear', 'tfidf__max_df': 0.9, 'tfidf__max_features': 5000}\n",
      "Best Cross-Validation Accuracy for svm: 0.5883438818565402\n",
      "Running GridSearch for naive_bayes...\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Best Parameters for naive_bayes: {'classifier__alpha': 0.01, 'tfidf__max_df': 0.8, 'tfidf__max_features': 5000}\n",
      "Best Cross-Validation Accuracy for naive_bayes: 0.5420358649789029\n",
      "Running GridSearch for gradient_boosting...\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "Best Parameters for gradient_boosting: {'classifier__learning_rate': 0.2, 'classifier__max_depth': 5, 'classifier__n_estimators': 100, 'tfidf__max_df': 0.8, 'tfidf__max_features': None}\n",
      "Best Cross-Validation Accuracy for gradient_boosting: 0.5129746835443038\n"
     ]
    }
   ],
   "source": [
    "tokenizers = [thai_preprocess, wangchan]\n",
    "# Create a dictionary to store the results\n",
    "results_dict = {}\n",
    "\n",
    "for tokenizer in tokenizers:\n",
    "    X = df['abstract'].apply(tokenizer)\n",
    "    y = df['category_id']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    model_name = tokenizer.__name__\n",
    "    results_dict[model_name] = grid_search(X_train, X_test, y_train, y_test, tokenizer)\n",
    "\n",
    "# Combine the results into one DataFrame\n",
    "combined_results = pd.concat(results_dict.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "081544b1-0eba-4c87-a950-24da7816135a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__kernel</th>\n",
       "      <th>param_tfidf__max_df</th>\n",
       "      <th>param_tfidf__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>param_classifier__alpha</th>\n",
       "      <th>param_classifier__learning_rate</th>\n",
       "      <th>param_classifier__max_depth</th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.236482</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>0.078355</td>\n",
       "      <td>0.005809</td>\n",
       "      <td>1.0</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__kernel': 'li...</td>\n",
       "      <td>0.525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.525211</td>\n",
       "      <td>0.00517</td>\n",
       "      <td>13</td>\n",
       "      <td>svm</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.228827</td>\n",
       "      <td>0.016699</td>\n",
       "      <td>0.066779</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7000</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__kernel': 'li...</td>\n",
       "      <td>0.525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.525211</td>\n",
       "      <td>0.00517</td>\n",
       "      <td>13</td>\n",
       "      <td>svm</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.202313</td>\n",
       "      <td>0.014212</td>\n",
       "      <td>0.067912</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>1.0</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.8</td>\n",
       "      <td>None</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__kernel': 'li...</td>\n",
       "      <td>0.525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.525211</td>\n",
       "      <td>0.00517</td>\n",
       "      <td>13</td>\n",
       "      <td>svm</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.225016</td>\n",
       "      <td>0.008092</td>\n",
       "      <td>0.071555</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__kernel': 'li...</td>\n",
       "      <td>0.525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.525211</td>\n",
       "      <td>0.00517</td>\n",
       "      <td>13</td>\n",
       "      <td>svm</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.204003</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.079748</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>1.0</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.9</td>\n",
       "      <td>7000</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__kernel': 'li...</td>\n",
       "      <td>0.525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.525211</td>\n",
       "      <td>0.00517</td>\n",
       "      <td>13</td>\n",
       "      <td>svm</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.236482      0.017402         0.078355        0.005809   \n",
       "1       0.228827      0.016699         0.066779        0.002901   \n",
       "2       0.202313      0.014212         0.067912        0.002721   \n",
       "3       0.225016      0.008092         0.071555        0.004083   \n",
       "4       0.204003      0.002104         0.079748        0.004877   \n",
       "\n",
       "   param_classifier__C param_classifier__kernel  param_tfidf__max_df  \\\n",
       "0                  1.0                   linear                  0.8   \n",
       "1                  1.0                   linear                  0.8   \n",
       "2                  1.0                   linear                  0.8   \n",
       "3                  1.0                   linear                  0.9   \n",
       "4                  1.0                   linear                  0.9   \n",
       "\n",
       "  param_tfidf__max_features  \\\n",
       "0                      5000   \n",
       "1                      7000   \n",
       "2                      None   \n",
       "3                      5000   \n",
       "4                      7000   \n",
       "\n",
       "                                              params  split0_test_score  ...  \\\n",
       "0  {'classifier__C': 1, 'classifier__kernel': 'li...              0.525  ...   \n",
       "1  {'classifier__C': 1, 'classifier__kernel': 'li...              0.525  ...   \n",
       "2  {'classifier__C': 1, 'classifier__kernel': 'li...              0.525  ...   \n",
       "3  {'classifier__C': 1, 'classifier__kernel': 'li...              0.525  ...   \n",
       "4  {'classifier__C': 1, 'classifier__kernel': 'li...              0.525  ...   \n",
       "\n",
       "   split2_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0           0.531646         0.525211         0.00517               13   \n",
       "1           0.531646         0.525211         0.00517               13   \n",
       "2           0.531646         0.525211         0.00517               13   \n",
       "3           0.531646         0.525211         0.00517               13   \n",
       "4           0.531646         0.525211         0.00517               13   \n",
       "\n",
       "   classifier        tokenizer param_classifier__alpha  \\\n",
       "0         svm  thai_preprocess                     NaN   \n",
       "1         svm  thai_preprocess                     NaN   \n",
       "2         svm  thai_preprocess                     NaN   \n",
       "3         svm  thai_preprocess                     NaN   \n",
       "4         svm  thai_preprocess                     NaN   \n",
       "\n",
       "   param_classifier__learning_rate  param_classifier__max_depth  \\\n",
       "0                              NaN                          NaN   \n",
       "1                              NaN                          NaN   \n",
       "2                              NaN                          NaN   \n",
       "3                              NaN                          NaN   \n",
       "4                              NaN                          NaN   \n",
       "\n",
       "   param_classifier__n_estimators  \n",
       "0                             NaN  \n",
       "1                             NaN  \n",
       "2                             NaN  \n",
       "3                             NaN  \n",
       "4                             NaN  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ea6ff12-d37c-4104-b980-079ab6da0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with No Max Features\n",
    "combined_results['param_tfidf__max_features'] = np.where(combined_results['param_tfidf__max_features'].isnull()==True, 'No Max Features',combined_results['param_tfidf__max_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2412fb-cdb7-40cc-a02e-3a3b591d43e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results.to_csv('../data/gridsearch_score.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
