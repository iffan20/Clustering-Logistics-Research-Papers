{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda8fea7-595c-4dbb-bde7-4730ee5edd06",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 100px\">\n",
    "\n",
    "# Capstone Project: Classifying Logistics Research Papers\n",
    "## Part 5 : Neural Network Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e4c0f-4754-4c61-b3fb-1efbec4b3069",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    " [Part 1: Get Text](01.Get_Text.ipynb) | [Part 2: Add Label](02.Add_Label.ipynb) | [Part 3: EDA](03.EDA.ipynb) | [Part 4: Gridsearch Classification](04.Gridsearch_Classification.ipynb) | **Part 5: Neural Network Classification** | [Part 6: Model Evaluation](06.Model_Evaluation.ipynb) | [Part 7: Final Model](07.Final_Model.ipynb) \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1c86d-c536-446b-988b-fc53bb970bb5",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook focuses on the model tuning and optimization process for a text classification task using a Neural Network model, while also trying to use under-sampling to handle imbalanced classes.\n",
    "\n",
    "To preprocess the text data, we use **TfidfVectorizer** as the vectorizer, which converts text into numerical features by calculating the Term Frequency-Inverse Document Frequency (TF-IDF) scores for each word. The TfidfVectorizer is further tuned as part of the GridSearch process by adjusting its key hyperparameters, such as `max_features` and `max_df`, to optimize the quality and efficiency of the text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff75d0-51d4-42bc-9b3f-d3c5d07f976e",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137d5c41-2134-43e1-97b0-7e8f12d3b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus.common import thai_words, thai_stopwords\n",
    "from pythainlp.util import dict_trie\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # For early stopping implementation\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f8831a-feed-44ff-b136-bb082f715da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>abstract</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>multi_category</th>\n",
       "      <th>keywords</th>\n",
       "      <th>category_id</th>\n",
       "      <th>abstract_length</th>\n",
       "      <th>content_length</th>\n",
       "      <th>content_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>การจัดทำคู่มือขั้นตอนการดำเนินการการนำเข้าคราฟ...</td>\n",
       "      <td>ผู้วิจัยได้ตระหนักถึงความยุ่งยากของขั้นตอนการน...</td>\n",
       "      <td>การจัดทำคู่มือขั้นตอนการดำเนินการการนำเข้าคราฟ...</td>\n",
       "      <td>Import-Export and International Trade</td>\n",
       "      <td>{1: 'Import-Export and International Trade', 2...</td>\n",
       "      <td>คู่มือการนำเข้าคราฟท์เบียร์, การดำเนินงานตามมา...</td>\n",
       "      <td>5</td>\n",
       "      <td>859</td>\n",
       "      <td>11582</td>\n",
       "      <td>2402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>การเสนอแนวทางในการพัฒนาและสร้างความสัมพันธ์กับ...</td>\n",
       "      <td>งานวิจัยครั้งนี้มีวัตถุประสงค์เพื่อเสนอแนวทางใ...</td>\n",
       "      <td>การเสนอแนวทางในการพัฒนาและสร้างความสัมพันธ์กับ...</td>\n",
       "      <td>Procurement</td>\n",
       "      <td>{1: 'Procurement', 2: 'Manufacturing/Productio...</td>\n",
       "      <td>การประเมินการปฏิบัติงาน, ผู้ส่งมอบ, แบ่งเกรด, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1172</td>\n",
       "      <td>15230</td>\n",
       "      <td>3601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>การพัฒนามาตรฐานรถขนส่งวัตถุอันตรายที่เข้ามาในค...</td>\n",
       "      <td>ดำเนินธุรกิจเป็นผู้นำเข้า และจัดจำหน่ายสินค้าก...</td>\n",
       "      <td>การพัฒนามาตรฐานรถขนส่งวัตถุอันตรายที่เข้ามาในค...</td>\n",
       "      <td>Logistics and Distribution</td>\n",
       "      <td>{1: 'Logistics and Distribution', 2: 'Inventor...</td>\n",
       "      <td>Chemical Solvent, การควบคุมความปลอดภัย, รถขนส่...</td>\n",
       "      <td>3</td>\n",
       "      <td>1964</td>\n",
       "      <td>13587</td>\n",
       "      <td>2883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>แนวทางการปรับปรุงกระบวนการการส่งเอกสารใบกำกับภ...</td>\n",
       "      <td>การวิจัยครั้งนี้มีวัตถุประสงค์ เพื่อศึกษาขั้นต...</td>\n",
       "      <td>แนวทางการปรับปรุงกระบวนการการส่งเอกสารใบกำกับภ...</td>\n",
       "      <td>Procurement</td>\n",
       "      <td>{1: 'Procurement', 2: 'Demand Planning and For...</td>\n",
       "      <td>การวิจัย, กระบวนการจัดส่งใบกำกับภาษี, แผนกบัญช...</td>\n",
       "      <td>0</td>\n",
       "      <td>1252</td>\n",
       "      <td>13124</td>\n",
       "      <td>3283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>การศึกษาเทคนิคการพยากรณ์ยอดขายสายไฟที่เหมาะสม</td>\n",
       "      <td>จากสถานการณ์การแพร่ระบาดของเชื้อไวรัสโคโรนา 20...</td>\n",
       "      <td>การศึกษาเทคนิคการพยากรณ์ยอดขายสายไฟที่เหมาะสม ...</td>\n",
       "      <td>Demand Planning and Forecasting</td>\n",
       "      <td>{1: 'Demand Planning and Forecasting', 2: 'Inv...</td>\n",
       "      <td>โควิด-19, ยานยนต์, การพยากรณ์ยอดขาย, Simple mo...</td>\n",
       "      <td>4</td>\n",
       "      <td>1924</td>\n",
       "      <td>25247</td>\n",
       "      <td>6114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             project  \\\n",
       "0  การจัดทำคู่มือขั้นตอนการดำเนินการการนำเข้าคราฟ...   \n",
       "1  การเสนอแนวทางในการพัฒนาและสร้างความสัมพันธ์กับ...   \n",
       "2  การพัฒนามาตรฐานรถขนส่งวัตถุอันตรายที่เข้ามาในค...   \n",
       "3  แนวทางการปรับปรุงกระบวนการการส่งเอกสารใบกำกับภ...   \n",
       "4      การศึกษาเทคนิคการพยากรณ์ยอดขายสายไฟที่เหมาะสม   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  ผู้วิจัยได้ตระหนักถึงความยุ่งยากของขั้นตอนการน...   \n",
       "1  งานวิจัยครั้งนี้มีวัตถุประสงค์เพื่อเสนอแนวทางใ...   \n",
       "2  ดำเนินธุรกิจเป็นผู้นำเข้า และจัดจำหน่ายสินค้าก...   \n",
       "3  การวิจัยครั้งนี้มีวัตถุประสงค์ เพื่อศึกษาขั้นต...   \n",
       "4  จากสถานการณ์การแพร่ระบาดของเชื้อไวรัสโคโรนา 20...   \n",
       "\n",
       "                                             content  \\\n",
       "0  การจัดทำคู่มือขั้นตอนการดำเนินการการนำเข้าคราฟ...   \n",
       "1  การเสนอแนวทางในการพัฒนาและสร้างความสัมพันธ์กับ...   \n",
       "2  การพัฒนามาตรฐานรถขนส่งวัตถุอันตรายที่เข้ามาในค...   \n",
       "3  แนวทางการปรับปรุงกระบวนการการส่งเอกสารใบกำกับภ...   \n",
       "4  การศึกษาเทคนิคการพยากรณ์ยอดขายสายไฟที่เหมาะสม ...   \n",
       "\n",
       "                                category  \\\n",
       "0  Import-Export and International Trade   \n",
       "1                            Procurement   \n",
       "2             Logistics and Distribution   \n",
       "3                            Procurement   \n",
       "4        Demand Planning and Forecasting   \n",
       "\n",
       "                                      multi_category  \\\n",
       "0  {1: 'Import-Export and International Trade', 2...   \n",
       "1  {1: 'Procurement', 2: 'Manufacturing/Productio...   \n",
       "2  {1: 'Logistics and Distribution', 2: 'Inventor...   \n",
       "3  {1: 'Procurement', 2: 'Demand Planning and For...   \n",
       "4  {1: 'Demand Planning and Forecasting', 2: 'Inv...   \n",
       "\n",
       "                                            keywords  category_id  \\\n",
       "0  คู่มือการนำเข้าคราฟท์เบียร์, การดำเนินงานตามมา...            5   \n",
       "1  การประเมินการปฏิบัติงาน, ผู้ส่งมอบ, แบ่งเกรด, ...            0   \n",
       "2  Chemical Solvent, การควบคุมความปลอดภัย, รถขนส่...            3   \n",
       "3  การวิจัย, กระบวนการจัดส่งใบกำกับภาษี, แผนกบัญช...            0   \n",
       "4  โควิด-19, ยานยนต์, การพยากรณ์ยอดขาย, Simple mo...            4   \n",
       "\n",
       "   abstract_length  content_length  content_word_count  \n",
       "0              859           11582                2402  \n",
       "1             1172           15230                3601  \n",
       "2             1964           13587                2883  \n",
       "3             1252           13124                3283  \n",
       "4             1924           25247                6114  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned_text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9225341e-b896-4641-9d4f-0518cb2923ee",
   "metadata": {},
   "source": [
    "### Text Preprocessing with Thai-Specific Methods\n",
    "\n",
    "\n",
    "Thai-specific text preprocessing methods for Natural Language Processing (NLP) tasks. It includes:\n",
    "\n",
    "- `Traditional Preprocessing` using stopword removal and tokenization tailored for Thai text.\n",
    "- `WangchanBERTa` Tokenization, leveraging a pre-trained Thai language model for deep learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9876ba9d-73e9-45d6-84f4-f6108d459237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom word to keep from 5 sample abstract\n",
    "added_words = ['การนำเข้า', 'ฐานนิยม', 'คราฟท์', 'แนวทาง', 'ผู้ส่งมอบ', 'โซ่อุปทาน', 'ปัจจัยรอง', \n",
    "               'การส่งมอบ', 'รถขนส่ง', 'นำไปใช้งาน', 'อย่างถูกต้อง', 'การขับรถ', 'ที่เกี่ยวข้อง', \n",
    "               'ในการปฏิบัติงาน', 'พนักงานขับรถ', 'สิ่งสำคัญ', 'ขั้นตอน', 'ที่ชัดเจน', 'การไหล', 'ยอดขาย', \n",
    "              'การจัดทำ', 'คราฟท์เบียร์', 'ฝึกสหกิจ', 'อย่างก้าวกระโดด','การจัดซื้อจัดหา','กระบวนการ',\n",
    "               'แบบประเมิน','เก็บข้อมูล','อย่างชัดเจน','การดำเนินการ','การส่งเสริม','ถังดับเพลิง','แนวทาง']\n",
    "\n",
    "# Merge custom words with Thai dictionary words\n",
    "custom_words = set(thai_words()).union(added_words)\n",
    "custom_trie = dict_trie(custom_words)  # Create a trie from the custom dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d58e87-0e84-455d-9be8-bb13f7f3a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = thai_stopwords()\n",
    "\n",
    "def thai_preprocess(text):\n",
    "    tokens = word_tokenize(text, custom_dict= custom_trie, engine=\"newmm\")\n",
    "    return \" \".join([token for token in tokens if token not in stopwords])\n",
    "\n",
    "def wangchan(text):   \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d54b3fd-08c3-4d59-bbda-7abeef43028f",
   "metadata": {},
   "source": [
    "### Neural Networks Model with Tokenizer and Vectorizer Parameter Tuning\n",
    "This process is to optimize a Neural Network model for text classification by experimenting with different tokenization methods and TF-IDF vectorizer hyperparameters. By systematically testing various combinations of tokenizers and vectorizer settings.\n",
    "\n",
    "**Setting Vectorizer Parameters** : We tune the TfidfVectorizer by optimizing the following two hyperparameters:\n",
    "\n",
    "1. `max_features` : Set to 5000, 7000, or None\n",
    "2. `max_df` : Set to 0.8 or 0.9\n",
    "\n",
    "Given that our dataset contains approximately 10,000 features per file, reducing the number of features may improve the model's performance by focusing on the most informative terms and reducing noise. This step is crucial for balancing model complexity and computational efficiency.\n",
    "\n",
    "**Handling Class Imbalance** : Since class imbalance can significantly affect the performance of classification models, we address this issue using `random undersampling`. By reducing the instances of the majority class, we create a more balanced dataset, which allows the model to better evaluate and differentiate between classes. This approach will be compared against the base model (without undersampling) during the evaluation process to determine whether it results in improved performance. The goal is to identify the best combination of tokenizer, vectorizer parameters, and data handling techniques for the classification task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886ab2b0-9400-4d47-90d1-b5a49cfd3255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seting List of params\n",
    "tokenizer = [thai_preprocess, wangchan]\n",
    "max_features = [5000,7000, None]\n",
    "max_df = [0.8, 0.9]\n",
    "total = len(tokenizer)*len(max_features)*len(max_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d6af5a-dc06-4f40-9c03-b626eedee8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fuction to fit Neural Network model\n",
    "def nn_model(X_train_tfidf, X_test_tfidf, y_train_encoded, y_test_encoded,shape):\n",
    "    \n",
    "    # Early stop\n",
    "    es = EarlyStopping(\n",
    "        monitor = 'val_loss'\n",
    "        , patience = 5\n",
    "        , restore_best_weights = True \n",
    "    )\n",
    "    \n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Input(shape=(shape,)))\n",
    "    nn_model.add(BatchNormalization())\n",
    "    \n",
    "    nn_model.add(Dense(512, activation='relu')) # 512 neurons with ReLU activation\n",
    "    nn_model.add(Dropout(0.2)) #  Dropout layers (with a rate of 0.2)\n",
    "    \n",
    "    nn_model.add(Dense(256, activation='relu')) # 256 neurons with ReLU activation\n",
    "    nn_model.add(Dropout(0.2)) #  Dropout layers (with a rate of 0.2)\n",
    "    \n",
    "    nn_model.add(Dense(128, activation='relu')) # 128 neurons with ReLU activation\n",
    "    nn_model.add(Dropout(0.2)) #  Dropout layers (with a rate of 0.2)\n",
    "    \n",
    "    nn_model.add(Dense(64, activation='relu')) # 64 neurons with ReLU activation\n",
    "    nn_model.add(Dense(32, activation='relu')) # 32 neurons with ReLU activation\n",
    "    \n",
    "    nn_model.add(Dense(len(df['category'].unique()), activation='softmax'))  # Softmax for multi-class classification / 8 classes\n",
    "    \n",
    "    # model compile\n",
    "    nn_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # train model\n",
    "    nn_model.fit(X_train_tfidf, y_train_encoded, epochs=50, batch_size=4, validation_data=(X_test_tfidf, y_test_encoded), callbacks = [es], verbose =0)\n",
    "    \n",
    "    # predict\n",
    "    y_pred_nn = nn_model.predict(X_test_tfidf, verbose =0)\n",
    "    y_pred_nn = y_pred_nn.argmax(axis=1)  # transform predict value to class\n",
    "    \n",
    "    # transform y back to value\n",
    "    y_pred_nn = label_encoder.inverse_transform(y_pred_nn)\n",
    "    \n",
    "    return accuracy_score(y_test, y_pred_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf4963-3d3c-4d36-aa18-9302b368a097",
   "metadata": {},
   "source": [
    "### Tuning Model with Different Parameters\n",
    "Since the score may vary after tuning the model, I will run the model 3 times and calculate the average score to evaluate its performance. Additionally, I will incorporate address class imbalance to ensure a fair and robust evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "717d0510-ae45-434a-acb1-25fcdca2fede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model tuning complete! Total runtime = 67 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "start_time = time.time()\n",
    "nn_score_list = []\n",
    "\n",
    "for tok in tokenizer:\n",
    "    for max_f in max_features:\n",
    "        for max_d in max_df:  \n",
    "            # Preprocess data\n",
    "\n",
    "            X = df['content'].apply(tok)\n",
    "            y = df['category_id']\n",
    "\n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "            # Apply TF-IDF\n",
    "            tfidf = TfidfVectorizer(max_features=max_f, max_df=max_d)\n",
    "            X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "            X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "            # transform y_train and y_test to numeric\n",
    "            label_encoder = LabelEncoder()\n",
    "            y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "            y_test_encoded = label_encoder.transform(y_test)\n",
    "    \n",
    "            # Get input shape for the NN model\n",
    "            shape = X_train_tfidf.shape[1]\n",
    "\n",
    "            # Run nn_model 5 times and store accuracies\n",
    "            accuracies = []\n",
    "            for _ in range(3):\n",
    "                acc = nn_model(X_train_tfidf, X_test_tfidf, y_train_encoded, y_test_encoded, shape)\n",
    "                accuracies.append(acc)\n",
    "            \n",
    "            # Compute mean accuracy\n",
    "            mean_acc = sum(accuracies) / len(accuracies)\n",
    "            \n",
    "            # Append results\n",
    "            nn_score_list.append({\n",
    "                'classifier': 'neural network',\n",
    "                'tokenizer': tok.__name__,\n",
    "                'max_features': max_f,\n",
    "                'max_df': max_d,\n",
    "                'accuracy': mean_acc\n",
    "            })\n",
    "\n",
    "runtime = (time.time() - start_time)/60\n",
    "\n",
    "print(f'All model tuning complete! Total runtime = {runtime:.0f} minutes.')\n",
    "# Convert to DataFrame\n",
    "nn_score_df = pd.DataFrame(nn_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5974f21-328b-454c-946c-b155ab0acb26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_df</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neural network</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.735294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neural network</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.725490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neural network</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.715686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neural network</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>No Max Features</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.696078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neural network</td>\n",
       "      <td>wangchan</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.696078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neural network</td>\n",
       "      <td>wangchan</td>\n",
       "      <td>No Max Features</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.696078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neural network</td>\n",
       "      <td>wangchan</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.676471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neural network</td>\n",
       "      <td>wangchan</td>\n",
       "      <td>No Max Features</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.676471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neural network</td>\n",
       "      <td>wangchan</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neural network</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>No Max Features</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>neural network</td>\n",
       "      <td>wangchan</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>neural network</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.627451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        classifier        tokenizer     max_features  max_df  accuracy\n",
       "0   neural network  thai_preprocess             5000     0.8  0.735294\n",
       "1   neural network  thai_preprocess             5000     0.9  0.725490\n",
       "2   neural network  thai_preprocess             7000     0.9  0.715686\n",
       "3   neural network  thai_preprocess  No Max Features     0.9  0.696078\n",
       "4   neural network         wangchan             7000     0.8  0.696078\n",
       "5   neural network         wangchan  No Max Features     0.9  0.696078\n",
       "6   neural network         wangchan             5000     0.8  0.676471\n",
       "7   neural network         wangchan  No Max Features     0.8  0.676471\n",
       "8   neural network         wangchan             7000     0.9  0.666667\n",
       "9   neural network  thai_preprocess  No Max Features     0.8  0.647059\n",
       "10  neural network         wangchan             5000     0.9  0.647059\n",
       "11  neural network  thai_preprocess             7000     0.8  0.627451"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_score_df = nn_score_df.sort_values(by = 'accuracy', ascending = False)\n",
    "\n",
    "# Replace NaN with No Max Features\n",
    "nn_score_df['max_features'] = np.where(nn_score_df['max_features'].isnull()==True, 'No Max Features',nn_score_df['max_features'])\n",
    "\n",
    "nn_score_df.to_csv('../data/nn_score.csv', index = False)\n",
    "nn_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "faf3e0c7-585a-45c3-a0b2-ed867fd13107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.6838235294117648\n",
      "\n",
      "Mean score by tokenizer\n",
      "thai_preprocess    0.691176\n",
      "wangchan           0.676471\n",
      "Name: accuracy, dtype: float64\n",
      "\n",
      "Mean score by max_features\n",
      "5000               0.696078\n",
      "7000               0.676471\n",
      "No Max Features    0.678922\n",
      "Name: accuracy, dtype: float64\n",
      "\n",
      "Mean score by max_df\n",
      "0.8    0.676471\n",
      "0.9    0.691176\n",
      "Name: accuracy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Mean score \n",
    "mean_score = float(nn_score_df['accuracy'].mean())\n",
    "print('Mean score:',mean_score)\n",
    "# Check performance by paramerters\n",
    "print('\\nMean score by',nn_score_df.groupby('tokenizer')['accuracy'].mean())\n",
    "print('\\nMean score by',nn_score_df.groupby('max_features')['accuracy'].mean())\n",
    "print('\\nMean score by',nn_score_df.groupby('max_df')['accuracy'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d1cbff-ad63-4364-8ed0-e1b7166b9fc9",
   "metadata": {},
   "source": [
    "### Try Handling Imbalanced Classes with Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bc147b3a-dda4-4591-8c23-8aaff90ac53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model tuning complete! Total runtime = 72 minutes.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "nn_uamp_score_list = []\n",
    "\n",
    "for tok in tokenizer:\n",
    "    for max_f in max_features:\n",
    "        for max_d in max_df:  \n",
    "            \n",
    "            # Preprocess data\n",
    "            X = df['content'].apply(tok)\n",
    "            y = df['category_id']\n",
    "\n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "            # Apply TF-IDF\n",
    "            tfidf = TfidfVectorizer(max_features=max_f, max_df=max_d)\n",
    "            X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "            X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "            # transform y_train and y_test to numeric\n",
    "            label_encoder = LabelEncoder()\n",
    "            y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "            y_test_encoded = label_encoder.transform(y_test)\n",
    "    \n",
    "            # Get input shape for the NN model\n",
    "            shape = X_train_tfidf.shape[1]\n",
    "\n",
    "            # Apply RandomUnderSampler\n",
    "            rus = RandomUnderSampler(random_state=42)\n",
    "            X_train_resampled, y_train_resampled = rus.fit_resample(X_train_tfidf, y_train_encoded)\n",
    "            \n",
    "            # Convert sparse matrices to dense arrays\n",
    "            X_train_resampled = X_train_resampled.toarray()\n",
    "            X_test_tfidf = X_test_tfidf.toarray()\n",
    "\n",
    "            # Run nn_model 3 times and store accuracies\n",
    "            accuracies = []\n",
    "            for _ in range(3):\n",
    "                acc = nn_model(X_train_resampled, X_test_tfidf, y_train_resampled, y_test_encoded, shape)\n",
    "                accuracies.append(acc)\n",
    "            \n",
    "            # Compute mean accuracy\n",
    "            mean_acc = sum(accuracies) / len(accuracies)\n",
    "            \n",
    "            # Append results\n",
    "            nn_uamp_score_list.append({\n",
    "                'classifier': 'neural network',\n",
    "                'tokenizer': tok.__name__,\n",
    "                'max_features': max_f,\n",
    "                'max_df': max_d,\n",
    "                'accuracy': mean_acc\n",
    "            })\n",
    "\n",
    "runtime = (time.time() - start_time)/60\n",
    "\n",
    "print(f'All model tuning complete! Total runtime = {runtime:.0f} minutes.')\n",
    "\n",
    "# Convert to DataFrame\n",
    "nn_usamp_score_df = pd.DataFrame(model_params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d482c60-0612-49d1-aa44-db86ab1598d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_df</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neural network</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neural network</td>\n",
       "      <td>wangchan</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.578431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neural network</td>\n",
       "      <td>wangchan</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.571895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>neural network</td>\n",
       "      <td>wangchan</td>\n",
       "      <td>No Max Features</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.571895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neural network</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>No Max Features</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.565359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neural network</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neural network</td>\n",
       "      <td>wangchan</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.542484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neural network</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.539216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neural network</td>\n",
       "      <td>wangchan</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.539216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neural network</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>No Max Features</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.535948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>neural network</td>\n",
       "      <td>wangchan</td>\n",
       "      <td>No Max Features</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.516340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neural network</td>\n",
       "      <td>thai_preprocess</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.483660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        classifier     tokenization     max_features  max_df  accuracy\n",
       "1   neural network  thai_preprocess           5000.0     0.9  0.588235\n",
       "9   neural network         wangchan           7000.0     0.9  0.578431\n",
       "6   neural network         wangchan           5000.0     0.8  0.571895\n",
       "11  neural network         wangchan  No Max Features     0.9  0.571895\n",
       "5   neural network  thai_preprocess  No Max Features     0.9  0.565359\n",
       "3   neural network  thai_preprocess           7000.0     0.9  0.555556\n",
       "8   neural network         wangchan           7000.0     0.8  0.542484\n",
       "0   neural network  thai_preprocess           5000.0     0.8  0.539216\n",
       "7   neural network         wangchan           5000.0     0.9  0.539216\n",
       "4   neural network  thai_preprocess  No Max Features     0.8  0.535948\n",
       "10  neural network         wangchan  No Max Features     0.8  0.516340\n",
       "2   neural network  thai_preprocess           7000.0     0.8  0.483660"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_usamp_score_df = nn_usamp_score_df.sort_values(by = 'accuracy', ascending = False)\n",
    "\n",
    "# Replace NaN with No Max Features\n",
    "nn_usamp_score_df['max_features'] = np.where(nn_usamp_score_df['max_features'].isnull()==True, 'No Max Features',nn_usamp_score_df['max_features'])\n",
    "\n",
    "nn_usamp_score_df.to_csv('../data/nn_usamp_score.csv', index = False)\n",
    "nn_usamp_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c4cd6238-6944-4428-b555-5d6113547e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.5490196078431372\n",
      "\n",
      "Mean score by tokenizer\n",
      "thai_preprocess    0.544662\n",
      "wangchan           0.553377\n",
      "Name: accuracy, dtype: float64\n",
      "\n",
      "Mean score by max_features\n",
      "5000.0             0.559641\n",
      "7000.0             0.540033\n",
      "No Max Features    0.547386\n",
      "Name: accuracy, dtype: float64\n",
      "\n",
      "Mean score by max_df\n",
      "0.8    0.531590\n",
      "0.9    0.566449\n",
      "Name: accuracy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Mean score \n",
    "usamp_mean_score = float(nn_usamp_score_df['accuracy'].mean())\n",
    "print('Mean score:',usamp_mean_score)\n",
    "# Check performance by paramerters\n",
    "print('\\nMean score by',nn_usamp_score_df.groupby('tokenizer')['accuracy'].mean())\n",
    "print('\\nMean score by',nn_usamp_score_df.groupby('max_features')['accuracy'].mean())\n",
    "print('\\nMean score by',nn_usamp_score_df.groupby('max_df')['accuracy'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d37fb-7dbf-4327-81dc-76a094609248",
   "metadata": {},
   "source": [
    "### Best Parameters for Base Neural Network and Model with Random Undersampling:\n",
    "- **Tokenization**: `thai_preprocess` performs slightly better than wangchan.\n",
    "- **Max_features**: Setting `max_features = 5000` yields the best performance.\n",
    "- **Max_df**: Setting `max_df = 0.9` provides the best results.\n",
    "\n",
    "the base model achieved a mean accuracy score of 0.68, while the model with undersampling resulted in a lower mean accuracy score of 0.55.\n",
    "\n",
    "**Summary: The base neural network model performs better than the model with random undersampling.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
